---
title: "rum_scales"
author: "matthew"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

rumination scales:
CDRS
RRS
SARI
BIS
UPPS
SCARED
TRASH
PSWQ
rumVAS
Mindwandering
MEAQA

1. pull database with individual items and scores.

2. evaluate for out of range responses - and correct.

3. evaluate for missing responses  and adjust - likely interpolation

4. recode variable names if/as needed. often redcap labels do not align nicely with use of variables in analyses and databases

5. ascertain whether there is sufficient post-treatment data to look at change. i doubt it, but it is probably good to run the same steps for a post-tx db, as we can then estimate when in the R33 we could revisit that question.

6. run initial bivariate factor analysis of the rumination scales individual response items. We do not have enough people to do this for scale development, but we can do it for exploratory paper - purposes.

7. run initial correlation matrix of scales and subscales as they are now.

8. run PCA and alpha for TRASH to determine scale internal reliability.

9. others?


```{r}
library(tidyverse)

# grab just rumination scales & clean up a bit more
df <- read_csv("../r61_r33_all_scales_events.csv") %>%
  select(c(id,study,event,redcap_repeat_instrument,redcap_repeat_instance,status,
           matches("^demo"),matches("^cdrs"),matches("^rrs"),matches("^upps"),lack_premed_total,
           neg_urgency_total,sen_seeking_raw_total,lack_perserve_total,pos_urgency_total,
           matches("^scared"),matches("^trash"),tracking_rum_as_a_habit_baseline_complete,
           matches("^pswq"),matches("^sari"),anger_rumination_total,sadness_rumination_total,
           matches("^rum_vas"),rum_habit_vas_complete,matches("^meaq"),matches("^mwds"),
           matches("^bis"),matches("signature$"),child_signature_assent)) %>%

  # filter( (!is.na(child_signature_assent) | !is.na(ut_assent_signature) |  # get only those with assent signatures: 64
  #           !is.na(nch_assent_signature) | !is.na(osu_assent_signature)) | (study == "r61" & status == 1)  ) %>% #&
  
  # (!is.na(consent_parent_signature) | !is.na(ut_consent_parent_signature) |
  #    !is.na(nch_consent_parent_signature) | !is.na(osu_consent_parent_signature)))
  # filter(status == 1) %>%  # all records with 'active' status: 97
  
  mutate(pre_post_tx = ifelse(.$event %in% c("dx_1","dx_1_re","np_1","mri_1"),"pre","post"), .before=event) %>%
  select(-c(matches("_date$"),matches("_visit$"),matches("_note$"),matches("signature"),cdrs_interviewer_other)) %>%
  rename(upps_lack_premed_total=lack_premed_total,upps_neg_urgency_total=neg_urgency_total,
         upps_sen_seeking_raw_total=sen_seeking_raw_total,upps_lack_perserve_total=lack_perserve_total,
         upps_pos_urgency_total=pos_urgency_total,trash_complete=tracking_rum_as_a_habit_baseline_complete) %>%
  
  # make reverse scored items match between r61 & r33
  mutate(across(c(upps_03_seethings,upps_05_upsetunfinished,upps_06_stopandthink,
                  upps_09_hatetostop,upps_10_whattodo,upps_13_thinkhard,
                  upps_15_finish,upps_16_carefulapproach,upps_19_getthingsdone,
                  upps_22_getsthejobdone,upps_23_careful,upps_24_finish,
                  upps_25_knowwhattoexpect,upps_28_stopandthink,upps_29_choices), 
                ~ifelse(study=="r33",5-.x,.x))) %>% # reverse score upps items if r33 (r61 already reversed)
  mutate(across(c(pswq_02_dont_worry_things,pswq_07_easy_stop_worrying,
                  pswq_09_never_worry), 
                ~ifelse(study=="r33",3-.x,.x))) %>%
  mutate(across(c(bisbas_01_fam_most_important,bisbas_03_go_out_way_get_things,
                  bisbas_04_keep_at_doing_well,bisbas_05_willing_try_new,
                  bisbas_06_how_dress_important,bisbas_07_excited_ener_get_want,
                  bisbas_08_critique_hurts,bisbas_09_go_all_out,bisbas_10_do_stuff_just_fun,
                  bisbas_11_hard_do_things,bisbas_12_love_right_away,bisbas_13_worry_upset_some1_mad,
                  bisbas_14_see_opport_excited,bisbas_15_act_spur_moment,bisbas_16_work_up_unpleasant,
                  bisbas_17_wonder_ppl_actions,bisbas_18_good_thngs_aff_strong,
                  bisbas_19_worry_do_poorly,bisbas_20_crave_excite_new,
                  bisbas_21_no_holds_approach,bisbas_23_exciting_win_contest,
                  bisbas_24_worry_mistakes), 
                ~ifelse(study=="r33",5-.x,.x))) %>%
  mutate(rum_vas_02_control = 100 - rum_vas_02_control) %>%
  mutate(across(c(cdrs01s,cdrs02s,cdrs03s,cdrs04s,cdrs05s,cdrs06s,cdrs07s,cdrs08s,
                  cdrs09s,cdrs10s,cdrs11s,cdrs12s,cdrs13s,cdrs14s,cdrs15,cdrs16,cdrs17),
                ~replace(.,9,NA))) %>%
  select(-c(matches("^upps_reverse"),matches("^pswqc_reverse"),
            matches("^bisbas_reverse")))

df
```

plan to filter participants:
'status' field is unreliable, many participants are NA
signature field is a possibility, but this doesn't exist for r61
here's my plan:
  select all scale ITEMS
  if a participant has ANY data in ANY item, they get included
  presumably they were consented if they got sent questionnaires
  
  THEN for a given participant and scale, get item completion percentage
  
  
```{r}
source("items_pretx_full.r")

df_c <- df %>%
  select(c(id,study,pre_post_tx,event,redcap_repeat_instrument,
           redcap_repeat_instance,status,cdrs_interviewer,
           matches("^demo_"),items_pretx)) %>%
  # check if a row has any data, drop if not
  mutate(null_message = as.numeric(rowSums(!is.na(.[items_pretx])) == 0), .after = id) %>%  # 1 if no data
  filter(null_message != 1) %>%
  select(-null_message)

# case by case id changes, mostly dx redos
df_c[df_c$id=="4574_redo", "event"] = "dx_1_re"
df_c[df_c$id=="4574_redo", "id"] = "4574"
df_c[df_c$id=="4598_redo", "event"] = "dx_1_re"
df_c[df_c$id=="4598_redo", "id"] = "4598"
# 3524 dx2 has moms data apparently, remove moms & rename kids
df_c <- df_c %>% filter(!(id=="3524" & event=="dx_2"))
df_c[df_c$id=="3524 (followup dx, mom took his)", "id"] = "3524"
df_c <- df_c %>% filter(!id=="3566_001")  # this only has repeated mri_2 data for 3566, ok to remove i think
df_c <- df_c %>% filter(!id=="9999_4575")  # not sure what this record is, only dx1 & less informative than 4575
df_c <- df_c %>% filter(!id=="10020")
df_c
```

have a look at IDs
```{r}
unique(df_c$id)
```

```{r}
# df_c %>% filter(str_detect(id,"100148"))
```

how many unique participants
```{r}
df_c %>%
  select(id) %>%
  unique() %>%
  count()
```

descriptive stats table to look at ranges of items & scores -- compare to expected ranges
```{r}
library(vtable)
sumtable(df_c,out = "csv", file = "stat_sum_items.csv")
```

Which time points/events are we including in the factor analysis?

basically 2 time points: pre & post tx

at both pre & post tx there could be multiple observations of a given scale
e.g. RRS is supposed to be done at least 3 times pre-tx

If we just look at pre-tx, there can be still repeated measurements per participant which could muddy the factor analysis
i.e. for a given item can't parse out the subject-level variation with the variation accounted for by a latent factor
items are not independent measurements

to note, cdrs interviewer could also be a level to consider

solution 1: include only the first observation of each scale/item for pre-tx
  con: less data
  pro: cleaner interpretation
soultion 2: Multilevel Exploratory Factor Analysis, with a level for participant (and maybe cdrs interviewer?)
  
Ok so scott said to only include the first observation, or take an average

For the first attempt, I'll look at only pre-tx and average any repeated items
this should result in one row per participant (one observation per item)

we can then get amount of missing data fairly easily & impute if necessary (fa functions often have built in imputing)

EFA & PCA ought to be straightforward from there

have a look at baseline only
```{r}
df_pre <- df_c %>%
  filter(pre_post_tx=="pre") %>%
  select(-c(event,redcap_repeat_instrument,redcap_repeat_instance,cdrs_interviewer)) %>%
  
  # average each item by participant
  group_by(id) %>%
  summarise(
    across(c(study,pre_post_tx,status,matches("^demo")), ~first(.x)),
    across(items_pretx, ~round(mean(.x, na.rm = T),1))
    )

write_csv(df_pre, "rum_scales_pretx.csv")

df_pre
```

```{r}
sumtable(df_pre,out = "csv", file = "stat_sum_pretx.csv")
```

Ok so we got a cleaned up pre-tx dataset

now to look at just the items & evaluate missing data

```{r}
df_pre_items = df_pre %>%
  select(-c(pre_post_tx,status,matches("^demo")))

write_csv(df_pre_items, "rum_scales_pretx_items.csv")

df_pre_items
```

# of observations for each item
```{r}
df_pre_items %>%
  summarise_all(~sum(!is.na(.x)))
```

# of missings for each item
```{r}
df_pre_items %>%
  summarise_all(~sum(is.na(.x)))
```


and % for each item
```{r}
df_pre_items %>%
  summarise_all(~round(sum(!is.na(.x))/n(),2))
```

what is the % completion for each scale for each participant
```{r}
# df_pre_items %>%
#   rowwise() %>%
#   mutate(cdrs_p = round(sum(!is.na(c_across(any_of(matches("^cdrs"))))) / length(c_across(any_of(matches("^cdrs")))),2), .before = cdrs01s) %>%
#   mutate(rrs_p = round(sum(!is.na(c_across(any_of(matches("^rrs"))))) / length(c_across(any_of(matches("^rrs")))),2), .before = cdrs01s) %>%
#   mutate(upps_p = round(sum(!is.na(c_across(any_of(matches("^upps"))))) / length(c_across(any_of(matches("^upps")))),2), .before = cdrs01s) %>%
#   mutate(scared_p = round(sum(!is.na(c_across(any_of(matches("^scared"))))) / length(c_across(any_of(matches("^scared")))),2), .before = cdrs01s) %>%
#   mutate(trash_p = round(sum(!is.na(c_across(any_of(matches("^trash"))))) / length(c_across(any_of(matches("^trash")))),2), .before = cdrs01s) %>%
#   mutate(pswq_p = round(sum(!is.na(c_across(any_of(matches("^pswq"))))) / length(c_across(any_of(matches("^pswq")))),2), .before = cdrs01s) %>%
#   mutate(sari_p = round(sum(!is.na(c_across(any_of(matches("^sari"))))) / length(c_across(any_of(matches("^sari")))),2), .before = cdrs01s) %>%
#   mutate(rum_vas_p = round(sum(!is.na(c_across(any_of(matches("^rum_vas"))))) / length(c_across(any_of(matches("^rum_vas")))),2), .before = cdrs01s) %>%
#   mutate(meaq_p = round(sum(!is.na(c_across(any_of(matches("^meaq"))))) / length(c_across(any_of(matches("^meaq")))),2), .before = cdrs01s) %>%
#   mutate(mwds_p = round(sum(!is.na(c_across(any_of(matches("^mwds"))))) / length(c_across(any_of(matches("^mwds")))),2), .before = cdrs01s) %>%
#   mutate(bisbas_p = round(sum(!is.na(c_across(any_of(matches("^bisbas"))))) / length(c_across(any_of(matches("^bisbas")))),2), .before = cdrs01s) %>%
#   select(c(id,matches("_p$"))) %>%
#   write_csv("ptp_scale_completion.csv")
```

% missing for each ptp for all scales
```{r}
df_pre_items %>%
  rowwise() %>%
  mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
  select(c(id,missing)) %>%
  filter(missing<=0.08)
```


```{r}
# df_pre dems
df_pre %>%
  rowwise() %>%
  mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
  filter(missing<=0.08) %>%
  select(-c(missing)) %>%
  ungroup() %>%
  write_csv("rum_scale_demographics.csv")

```

for chuck
```{r}
df_fa = df_pre_items %>%
  rowwise() %>%
  mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
  filter(missing<=0.08) %>%
  select(-c(missing)) %>%
  ungroup() %>%
  mutate_all(~replace(., is.na(.), mean(., na.rm = T))) %>%
  write_csv("rum_items_ids.csv")
```

include participants with % missing data <= 0.08
resulting in 86 subjects
```{r}
df_fa = df_pre_items %>%
  rowwise() %>%
  mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
  filter(missing<=0.08) %>%
  select(-c(id,study,missing)) %>%
  ungroup()

df_fa
```
check for missing
```{r}
df_fa %>% filter(if_any(everything(), is.na))
```

single imputation (not needed for the subset of rum scales [rrs, upps, trash, pswq, sari, rum_vas] as these 86 ptps have no missing data for these scales)

fill any NA with the mean of all non-NA from that respective column

then scale (mean of zero & sd of 1)
```{r}
df_fa_imp = df_fa %>%
  mutate_all(~replace(., is.na(.), mean(., na.rm = T)))

df_fa_imp
```


double check column means
```{r}
df_fa %>%
  summarise_all(~mean(.,na.rm = T))
```

save data to csv
```{r}
df_fa_imp %>%
  write_csv("rum_items.csv")
```

```{r}
# rrs = df_fa_imp %>%
#   select(matches("^rrs"))
# 
# rrs.cor = cor(rrs)
```

scale data
```{r}
# df_imp_scal = df_fa_imp %>%
#   scale() %>%
#   as_tibble()

# write_csv(df_imp_scal, "rum_items_scaled.csv")
```






### post-tx

```{r}
source("items_posttx_full.r")

df_d <- df %>%
  select(c(id,study,pre_post_tx,event,redcap_repeat_instrument,
           redcap_repeat_instance,status,cdrs_interviewer,
           matches("^demo_"),items_posttx)) %>%
  # check if a row has any data, drop if not
  mutate(null_message = as.numeric(rowSums(!is.na(.[items_posttx])) == 0), .after = id) %>%  # 1 if no data
  filter(null_message != 1) %>%
  select(-null_message)

# case by case id changes, mostly dx redos
df_d[df_d$id=="4574_redo", "event"] = "dx_1_re"
df_d[df_d$id=="4574_redo", "id"] = "4574"
df_d[df_d$id=="4598_redo", "event"] = "dx_1_re"
df_d[df_d$id=="4598_redo", "id"] = "4598"
# 3524 dx2 has moms data apparently, remove moms & rename kids
df_d <- df_d %>% filter(!(id=="3524" & event=="dx_2"))
df_d[df_d$id=="3524 (followup dx, mom took his)", "id"] = "3524"
df_d <- df_d %>% filter(!id=="3566_001")  # this only has repeated mri_2 data for 3566, ok to remove i think
df_d <- df_d %>% filter(!id=="9999_4575")  # not sure what this record is, only dx1 & less informative than 4575
df_d <- df_d %>% filter(!id=="10020")
df_d
```

```{r}
df_post_trash = df_d %>%
  filter(pre_post_tx=="post") %>%
  select(c(id,study,pre_post_tx,event,matches("^trash"))) %>%
  group_by(event) %>%
  summarise_all(~sum(!is.na(.x)))

df_post_trash
```


```{r}
df_post <- df_d %>%
  filter(pre_post_tx=="post") %>%
  select(-c(redcap_repeat_instrument,redcap_repeat_instance,cdrs_interviewer)) %>%
  
  # average each item by participant
  group_by(id) %>%
  summarise(
    across(c(study,pre_post_tx,status,matches("^demo")), ~first(.x)),
    across(items_posttx, ~round(mean(.x, na.rm = T),1))
    )

#write_csv(df_pre, "rum_scales_pretx.csv")

df_post
```

```{r}
#sumtable(df_pre,out = "csv", file = "stat_sum_pretx.csv")
```

```{r}
df_post_items = df_post %>%
  select(-c(pre_post_tx,status,matches("^demo")))

#write_csv(df_pre_items, "rum_scales_pretx_items.csv")

df_post_items
```

# of observations for each item
```{r}
df_post_items %>%
  summarise_all(~sum(!is.na(.x))) %>%
  select(matches("^trash"))
```

```{r}
df_post_items %>%
  rowwise() %>%
  mutate(any_post_trash = ifelse(!is.nan(trash_01_automatically) | !is.nan(trashtx_01_automatically),1,0)) %>%
  select(c(any_post_trash)) %>%
  ungroup() %>%
  summarise_all(~sum(.x))

```


# of missings for each item
```{r}
df_post_items %>%
  summarise_all(~sum(is.na(.x)))
```


and % for each item
```{r}
df_post_items %>%
  summarise_all(~round(sum(!is.na(.x))/n(),2))
```

what is the % completion for each scale for each participant
```{r}
# df_pre_items %>%
#   rowwise() %>%
#   mutate(cdrs_p = round(sum(!is.na(c_across(any_of(matches("^cdrs"))))) / length(c_across(any_of(matches("^cdrs")))),2), .before = cdrs01s) %>%
#   mutate(rrs_p = round(sum(!is.na(c_across(any_of(matches("^rrs"))))) / length(c_across(any_of(matches("^rrs")))),2), .before = cdrs01s) %>%
#   mutate(upps_p = round(sum(!is.na(c_across(any_of(matches("^upps"))))) / length(c_across(any_of(matches("^upps")))),2), .before = cdrs01s) %>%
#   mutate(scared_p = round(sum(!is.na(c_across(any_of(matches("^scared"))))) / length(c_across(any_of(matches("^scared")))),2), .before = cdrs01s) %>%
#   mutate(trash_p = round(sum(!is.na(c_across(any_of(matches("^trash"))))) / length(c_across(any_of(matches("^trash")))),2), .before = cdrs01s) %>%
#   mutate(pswq_p = round(sum(!is.na(c_across(any_of(matches("^pswq"))))) / length(c_across(any_of(matches("^pswq")))),2), .before = cdrs01s) %>%
#   mutate(sari_p = round(sum(!is.na(c_across(any_of(matches("^sari"))))) / length(c_across(any_of(matches("^sari")))),2), .before = cdrs01s) %>%
#   mutate(rum_vas_p = round(sum(!is.na(c_across(any_of(matches("^rum_vas"))))) / length(c_across(any_of(matches("^rum_vas")))),2), .before = cdrs01s) %>%
#   mutate(meaq_p = round(sum(!is.na(c_across(any_of(matches("^meaq"))))) / length(c_across(any_of(matches("^meaq")))),2), .before = cdrs01s) %>%
#   mutate(mwds_p = round(sum(!is.na(c_across(any_of(matches("^mwds"))))) / length(c_across(any_of(matches("^mwds")))),2), .before = cdrs01s) %>%
#   mutate(bisbas_p = round(sum(!is.na(c_across(any_of(matches("^bisbas"))))) / length(c_across(any_of(matches("^bisbas")))),2), .before = cdrs01s) %>%
#   select(c(id,matches("_p$"))) %>%
#   write_csv("ptp_scale_completion.csv")
```

% missing for each ptp for all scales
```{r}
df_post_items %>%
  rowwise() %>%
  mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
  select(c(id,missing)) %>%
  filter(missing<=1.0)
```

# ```{r}
# df_fa = df_post_items %>%
#   rowwise() %>%
#   mutate(missing = round(sum(is.na(c_across(items_pretx))) / length(c_across(items_pretx)),2), .after=study) %>%
#   filter(missing<=0.08) %>%
#   select(-c(id,study,missing)) %>%
#   ungroup()
# 
# df_fa
# ```
# check for missing
# ```{r}
# df_fa %>% filter(if_any(everything(), is.na))
# ```
# 
# ```{r}
# df_fa_imp = df_fa %>%
#   mutate_all(~replace(., is.na(.), mean(., na.rm = T)))
# 
# df_fa_imp
# ```






## Factor Analysis

Bifactor analysis

1 all rumination vars
2 subset including rrs, trash, pswq, sari, rum vas

### all rumination items


Exploratory Bi-factor Analysis
https://pubmed.ncbi.nlm.nih.gov/22232562/#:~:text=Bi-factor%20analysis%20is%20a%20form%20of%20confirmatory%20factor,general%20factor%20and%20a%20number%20of%20group%20factors

Exploratory Bi-factor Analysis:The Oblique Case
https://escholarship.org/uc/item/3w14k56d


Using Exploratory Bifactor Analysis to Understand the Latent Structure of
Multidimensional Psychological Measures: An Example Featuring the WISC-V
Stefan C. Dombrowski et al

https://rjmcgill.com/wp-content/uploads/2019/07/ebfa-final-reprint.pdf

https://osf.io/5tdr2

psych biquartimin: implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011)
https://personality-project.org/r/psych/help/Promax.html


eigenvalues
```{r}
rum = df_fa_imp
rum.cor = cor(rum)  # pearson
#eigen(rum.cor)$values
```


```{r}
KMO(rum)
```

```{r}
bartlett.test(rum)
```

MAP and BIC
```{r}
library(psych)
nfactors(rum.cor, n.obs = 86)
```
parallel analysis--principal axis
```{r}
rum.pa.mean <- fa.parallel(
  rum.cor,
  n.obs = 86,
  fa = "both",
  fm = "minres")
```

regular old fa
```{r}
# f7.minres <- fa(
#   rum.cor,
#   nfactors = 11,
#   n.obs = 86,
#   rotate = "oblimin",
#   fm = "minres",
#   alpha=0.3
#   )
# 
# # f7.minres$loadings %>%
# #   as_tibble() %>%
# #   mutate(across(matches("^MR"), ~ifelse(.x>=0.3,.x,NA))) %>%
# #   print(sort = T)
# 
# f7.minres$loadings[f7.minres$loadings < 0.3] <- 0
# f7.minres$loadings

```

Factor extraction

try methods: pa,minres

maximum likelihood does not work

how to tell it was a good extraction?
look at communality estimates (h2)

```{r}
f11.minres.smc <- fa(
  rum.cor,
  nfactors = 11,
  n.obs = 86,
  rotate = "none",
  fm = "minres",
  max.iter = 100,
  SMC = T)

#f11.minres.smc
```

this one doesn't work
```{r}
# f11.pa.smc <- fa(
#   rum.cor,
#   nfactors = 11,
#   n.obs = 86,
#   rotate = "none",
#   fm = "pa",
#   max.iter = 100,
#   SMC = smc(rum.cor))
# 
# f11.pa.smc
```

```{r}
f11.pa.3 <- fa(
  rum.cor,
  nfactors = 11,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = rep(0.3, length(names(rum)))
  )

f11.pa.3
```

```{r}
# extract factor loadings
#f11.pa.smc.load <- f11.pa.smc$loadings
f11.pa.3.load <- f11.pa.3$loadings
```



Old orthogonal bifactor rotation
```{r}
# function to complete EBFA using arbitrary number of replications 
# taken from:
# Loehlin, J. C., & Beaujean, A. A. (2016). Syntax companion for Latent Variable Models: An Introduction yo Factor, Path, And Structural Equation Analysis (5th Ed.) (5th ed.). Waco, TX: Baylor Psychometric Laboratory.

# rotation options are bifactor (for biquartimin) and geomin (for bigeomin)
# FindBifactor.orth <- function(A,reps=10,rotation="bifactor",solutions=1,round=8,maxit=1000, seed=NA)
# {
# # A <- A[[1]] # this is only needed if come from psych package
# require(GPArotation)
# require(parallel)
# m <- dim(A)[2]
# seed <- ifelse(!is.na(seed),round(seed),ceiling(runif(1, 0, 10^9)))
# set.seed(seed)
# ran.mat <- replicate(reps,Random.Start(m),simplify=FALSE)
# y <- mclapply(ran.mat, function(z) GPForth(A,method=rotation,Tmat =z,maxit=maxit))  # GPForth
# y <- y[lapply(y, function(z) z$convergence) ==TRUE]
# criterion <- sapply(y, function(z) min(z$Table[,2]))
# results <- lapply(y, function(z) z$loadings)
# criterion <- round(criterion,round)
# results <- lapply(results, function(x) round(x,2))
# index.val <- 1:length(y)
# criterion.index <- data.frame(criterion,index.val)
# criterion.index <- criterion.index[order(criterion),]
# criterion.index.u <- criterion.index[match(unique(criterion.index$criterion), criterion.index$criterion),]
# index.keep <- criterion.index.u$index.val[1:solutions]
# output <- list(criterion=criterion[index.keep], loadings=results[index.keep])
# return(output)
# }
```


```{r}
# bi-quartimin, 1000 random starts, return up to the 10 best solutions
# biquart_full = FindBifactor.orth(
#   f11.pa.3.load,
#   reps = 1000,
#   rotation = "bifactor",
#   solutions = 10
# )
# 
# biquart_full
```


```{r}
# test = "./loadings_all_items/biquart.full.loadings"
# i = 1
# for (loadings in biquart_full$loadings) {
#   if(length(loadings) > 1) {
#     loadings[abs(loadings) < 0.3] <- NA
#     loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
#     print(paste("Solution ",i,sep = ""))
#     print(loadings)
#   }
#   i = i + 1
# }
```

```{r}
# biquart.loadings.1.full = biquart_full$loadings[[1]]
# biquart.loadings.1.full[abs(biquart.loadings.1.full) < 0.3] <- NA
# biquart.loadings.1.full %>% as.data.frame() %>% 
#   write.csv(paste("./loadings_all_items/biquart.loadings.1.",format(Sys.time(),"%Y%m%d%H%M%S"),".csv"))
# biquart.loadings.1.full
```


```{r}
# bi-geomin, 1000 random starts, return up to the 10 best solutions
# geomin_full = FindBifactor.orth(
#   f11.pa.3.load,
#   reps = 1000,
#   rotation = "geomin",
#   solutions = 10
# )
# 
# geomin_full
```

```{r}
# test = "./loadings_all_items/geomin.full.loadings"
# i = 1
# for (loadings in geomin_full$loadings) {
#   if(length(loadings) > 1) {
#     loadings[abs(loadings) < 0.3] <- NA
#     loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
#     print(paste("Solution ",i,sep = ""))
#     print(loadings)
#   }
#   i = i + 1
# }

```

```{r}
# geomin.loadings.1.full = geomin_full$loadings[[1]]
# geomin.loadings.1.full[abs(geomin.loadings.1.full) < 0.3] <- NA
# geomin.loadings.1.full %>% as.data.frame() %>% 
#   write.csv(paste("./loadings_all_items/geomin.loadings.1.",format(Sys.time(),"%Y%m%d%H%M%S"),".csv"))
# geomin.loadings.1.full
```



Oblique bifactor rotation
```{r}
# psych biquartimin: implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011)
# https://personality-project.org/r/psych/help/Promax.html
f11.psych.biquart <- psych::biquartimin(f11.pa.3.load, eps=1e-5, maxit=1000)
```

```{r}
f11.psych.biquart$loadings %>%
  as.data.frame() %>%
  apply(2, function(x) ifelse(abs(x) < 0.3, NA, x)) %>%
  apply(2, function(x) round(x,digits = 2)) %>%
  write.csv(paste("f11.psych.biquart",format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep="."))
```

# get factor scores & correlations
```{r}
f11.scores <- factor.scores(df_fa_imp, f11.psych.biquart$loadings, method = "Bartlett")
f11.scores.cor <- cor(f11.scores$scores,use = "pairwise")

library(ggcorrplot)
corr = round(f11.scores.cor,2)
pmat = cor_pmat(f11.scores$scores)
ggcorrplot(corr, type = "lower", hc.order = F, lab = T, lab_size = 3) +
  ggtitle("Factor score correlations")


```



<!---

### subset 1

eigenvalues
```{r}
source("items_pretx_sub1.r")
rum_sub1 = df_fa_imp %>% select(all_of(items_pretx_sub1))
rum_sub1.cor = cor(rum_sub1)
eigen(rum_sub1.cor)$values
```

```{r}
rum_sub1
```

MAP and BIC
```{r}
library(psych)
nfactors(rum_sub1.cor, n.obs = 86)
```
parallel analysis--principal axis
```{r}
rum.pa.mean <- fa.parallel(
  rum_sub1.cor,
  n.obs = 86,
  fa = "both",
  fm = "pa")
```

regular old fa
```{r}
f7.minres <- fa(
  rum_sub1.cor,
  nfactors = 7,
  n.obs = 86,
  rotate = "oblimin",
  fm = "minres",
  alpha=0.3
  )

# f7.minres$loadings %>%
#   as_tibble() %>%
#   mutate(across(matches("^MR"), ~ifelse(.x>=0.3,.x,NA))) %>%
#   print(sort = T)

f7.minres$loadings[f7.minres$loadings < 0.3] <- 0
f7.minres$loadings

```

```{r}
f7.minres.smc <- fa(
  rum_sub1.cor,
  nfactors = 7,
  n.obs = 86,
  rotate = "none",
  fm = "minres",
  max.iter = 100,
  SMC = T
)

f7.minres.smc
```

```{r}
f7.pa.smc <- fa(
  rum_sub1.cor,
  nfactors = 7,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = smc(rum_sub1.cor))

f7.pa.smc
```

```{r}
f7.pa.3 <- fa(
  rum_sub1.cor,
  nfactors = 7,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = rep(0.3, length(names(rum_sub1)))
  )

f7.pa.3
```

```{r}
# extract factor loadings
f7.pa.smc.load <- f7.pa.smc$loadings
f7.pa.3.load <- f7.pa.3$loadings
```


```{r}
# bi-quartimin, 1000 random starts, return up to the 10 best solutions
biquart_sub1 = FindBifactor.orth(
  f7.pa.smc.load,
  reps = 1000,
  rotation = "bifactor",
  solutions = 10
)

biquart_sub1
```

```{r}
test = "./loadings_sub1/biquart.sub1.loadings"
i = 1
for (loadings in biquart_sub1$loadings) {
  if(length(loadings) > 1) {
    loadings[abs(loadings) < 0.3] <- NA
    loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
    print(paste("Solution ",i,sep = ""))
    print(loadings)
  }
  i = i + 1
}
```

```{r}
# biquart.loadings.1 = biquart$loadings[[1]]
# biquart.loadings.1[abs(biquart.loadings.1) < 0.3] <- NA
# biquart.loadings.1 %>% as.data.frame() %>%
#   write.csv(paste("./loadings/biquart.loadings.1.",format(Sys.time(),"%Y%m%d%H%M%S"),".csv"))
# biquart.loadings.1
```


```{r}
# bi-geomin, 1000 random starts, return up to the 10 best solutions
geomin_sub1 = FindBifactor.orth(
  f7.pa.smc.load,
  reps = 1000,
  rotation = "geomin",
  solutions = 10
)

geomin_sub1
```

```{r}
test = "./loadings_sub1/geomin.sub1.loadings"
i = 1
for (loadings in geomin_sub1$loadings) {
  if(length(loadings) > 1) {
    loadings[abs(loadings) < 0.3] <- NA
    loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
    print(paste("Solution ",i,sep = ""))
    print(loadings)
  }
  i = i + 1
}
```

--->


### Subset

rrs,trash,sari,rum_vas,pswq

eigenvalues
```{r}
source("items_pretx_sub2.r")
rum_sub2 = df_fa_imp %>% select(all_of(items_pretx_sub2))
rumitems = rum_sub2
save(rumitems, file = "rumitems.RData")
rum_sub2.cor = cor(rum_sub2)
eigen(rum_sub2.cor)$values
```
86 x 69
```{r}
rum_sub2
```

Kaiser-Meyer-Olkin criterion
```{r}
KMO(rum_sub2)
```

```{r}
bartlett.test(rum_sub2)
```

```{r}
rum_sub3 = rum_sub2 %>%
  select(-c(rrs_12,trash_08_same_times_day,pswq_07_easy_stop_worrying,rum_vas_02_control,rum_vas_03_duration))
```

```{r}
KMO(rum_sub3)
```

MAP and BIC
```{r}
library(psych)
nfactors(rum_sub2.cor, n.obs = 86)
```

parallel analysis--principal axis
```{r}
rum.pa.mean <- fa.parallel(
  rum_sub2.cor,
  n.obs = 86,
  fa = "fa",
  fm = "pa")
```

Both BIC & Parallel Analysis suggest 5 factors

regular old oblique fa
```{r}
# f5.minres <- fa(
#   rum_sub2.cor,
#   nfactors = 5,
#   n.obs = 86,
#   rotate = "oblimin",
#   fm = "ml",
#   alpha=0.3
#   )
# 
# f5.minres$loadings[abs(f5.minres$loadings) < 0.3] <- 0
# f5.minres$loadings

```


# extract 5 factors
```{r}
f5.minres.smc <- fa(
  rum_sub2.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "none",
  fm = "minres",
  max.iter = 100,
  SMC = T
)

#f5.minres.smc
```

```{r}
f5.pa.smc <- fa(
  rum_sub2.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = smc(rum_sub2.cor))

#f5.pa.smc
```






```{r}
efa.2.final = fa(
  rum_sub2.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "biquartimin",
  fm = "pa",
  max.iter = 100,
  SMC = smc(rum_sub2.cor))
```

```{r}
efa.2.final$loadings %>%
  apply(2, function(x) round(x,digits = 2)) %>%
  write.csv("efa.2.final.csv")
```

```{r}
efa.2.final$uniquenesses %>%
  round(digits = 2) %>%
  write.csv("efa.2.uniq.csv")
```

```{r}
efa.2.scores <- factor.scores(rum_sub2, efa.2.final$loadings, method = "Bartlett")
efa.2.scores.cor <- cor(efa.2.scores$scores,use = "pairwise")
efa.2.scores.cor %>% write.csv("efa.2.scores.cor.csv")
```






```{r}
f5.pa.3 <- fa(
  rum_sub2.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = rep(0.3, length(names(rum_sub2)))
  )

#f5.pa.3
```

```{r}
f5.ml.smc <- fa(
  rum_sub2.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "none",
  fm = "ml",
  max.iter = 100,
  SMC = T)

#f5.ml.smc
```

```{r}
# extract factor loadings
f5.pa.smc.load <- f5.pa.smc$loadings
f5.pa.3.load <- f5.pa.3$loadings
f5.ml.smc.load <- f5.ml.smc$loadings
```

orthogonal bifactor rotation
```{r}
# bi-quartimin, 1000 random starts, return up to the 10 best solutions
# biquart_ml_sub2 = FindBifactor.orth(
#   f5.ml.smc.load,
#   reps = 1000,
#   rotation = "bifactor",
#   solutions = 10
# )
# 
# biquart_ml_sub2
```

```{r}
# test = "./loadings_sub2/biquart.ml.sub2.loadings"
# i = 1
# for (loadings in biquart_ml_sub2$loadings) {
#   if(length(loadings) > 1) {
#     loadings[abs(loadings) < 0.3] <- NA
#     loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
#     print(paste("Solution ",i,sep = ""))
#     print(loadings)
#   }
#   i = i + 1
# }
```

```{r}
# bi-quartimin, 1000 random starts, return up to the 10 best solutions
# biquart_oblq = FindBifactor.oblq(
#   f5.pa.smc.load,
#   reps = 1000,
#   rotation = "bifactor",
#   solutions = 10
# )
# 
# biquart_oblq
```

```{r}
# bi-quartimin, 1000 random starts, return up to the 10 best solutions
# biquart_sub2 = FindBifactor.orth(
#   f5.pa.smc.load,
#   reps = 1000,
#   rotation = "bifactor",
#   solutions = 10
# )
# 
# biquart_sub2
```
factor scores
```{r}
# f5.scores = factor.scores(rum_sub2, biquart_sub2$loadings[[1]], method = "Bartlett")
# f5.scores
```

```{r}
#cor(f5.scores$scores,use = "pairwise")
```


```{r}
# library(ggcorrplot)
# 
# corr = cor(f5.scores$scores)
# 
# pmat = cor_pmat(f5.scores$scores)
# ggcorrplot(corr, hc.order = F, lab = T, lab_size = 10)

```

```{r}
# test = "./loadings_sub2/biquart.sub2.smc.loadings"
# i = 1
# for (loadings in biquart_sub2$loadings) {
#   if(length(loadings) > 1) {
#     loadings[abs(loadings) < 0.3] <- NA
#     loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
#     print(paste("Solution ",i,sep = ""))
#     print(loadings)
#   }
#   i = i + 1
# }
```


```{r}
# bi-geomin, 1000 random starts, return up to the 10 best solutions
# geomin_sub2 = FindBifactor.orth(
#   f5.pa.smc.load,
#   reps = 1000,
#   rotation = "geomin",
#   solutions = 10
# )
# 
# geomin_sub2
```

```{r}
# test = "./loadings_sub2/geomin.sub2.smc.loadings"
# i = 1
# for (loadings in geomin_sub2$loadings) {
#   if(length(loadings) > 1) {
#     loadings[abs(loadings) < 0.3] <- NA
#     loadings %>% as.data.frame() %>% write.csv(paste(test,i,format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep = "."))
#     print(paste("Solution ",i,sep = ""))
#     print(loadings)
#   }
#   i = i + 1
# }
```


Oblique bifactor rotation
```{r}
# psych biquartimin: implements the oblique bifactor rotation introduced by Jennrich and Bentler (2011)
# https://personality-project.org/r/psych/help/Promax.html
f5.psych.biquart <- psych::biquartimin(f5.pa.smc.load, eps=1e-5, maxit=1000)
```

```{r}
f5.psych.biquart$loadings %>%
  as.data.frame() %>%
  #apply(2, function(x) ifelse(abs(x) < 0.3, NA, x)) %>%
  apply(2, function(x) round(x,digits = 2)) %>%
  write.csv(paste("f5.psych.biquart",format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep="."))
```

# get factor scores & correlations
```{r}
f5.scores <- factor.scores(rum_sub2, f5.psych.biquart$loadings, method = "Bartlett")
f5.scores.cor <- cor(f5.scores$scores,use = "pairwise")

library(ggcorrplot)
corr = round(f5.scores.cor,2)
pmat = cor_pmat(f5.scores$scores)
ggcorrplot(corr, type = "lower", hc.order = F, lab = T, lab_size = 5) +
  ggtitle("Factor score correlations")


```




```{r}
rum_sub3.cor = cor(rum_sub3)

rum.pa.mean <- fa.parallel(
  rum_sub3.cor,
  n.obs = 86,
  fa = "fa",
  fm = "pa")
```

```{r}
test.pa.smc <- fa(
  rum_sub3.cor,
  nfactors = 5,
  n.obs = 86,
  rotate = "none",
  fm = "pa",
  max.iter = 100,
  SMC = smc(rum_sub3.cor))

test.pa.smc.load <- test.pa.smc$loadings

test.psych.biquart <- psych::biquartimin(test.pa.smc.load, eps=1e-5, maxit=1000)

```

```{r}
test.psych.biquart$loadings %>%
  as.data.frame() %>%
  apply(2, function(x) ifelse(abs(x) < 0.3, NA, x)) %>%
  apply(2, function(x) round(x,digits = 2)) %>%
  write.csv(paste("test.psych.biquart",format(Sys.time(),"%Y%m%d%H%M%S"),"csv",sep="."))
```





```{r}
trash = df_fa_imp %>% select(matches("^trash"))
trash.cor = cor(trash)
eigen(trash.cor)$values
```

```{r}
KMO(trash)
```

```{r}
bartlett.test(trash)
```

```{r}
library(psych)
nfactors(trash.cor, n.obs = 86)
```

parallel analysis--principal axis
```{r}
trash.pa.mean <- fa.parallel(
  trash.cor,
  n.obs = 86,
  fa = "fa",
  fm = "pa")
```

```{r}
trash.pa.mean$fa.values
```

```{r}
trash.f2 <- fa(
  trash.cor,
  nfactors = 2,
  n.obs = 86,
  rotate = "oblimin",
  fm = "pa",
  max.iter = 100,
  SMC = smc(trash.cor))

trash.f2
```












## correlation matrices

all items correlation matrix
```{r,fig.height=40,fig.width=40}
library(ggcorrplot)

corr = round(cor(df_fa_imp),2)  # use = "pairwise.complete.obs" (for missing)
pmat = cor_pmat(df_fa_imp)
ggcorrplot(corr, type = "lower")

```

trash
```{r}
df_trash = df_fa_imp %>%
  dplyr::select(matches("^trash"))

corr_trash = round(cor(df_trash),2)  # use = "pairwise.complete.obs" (for missing)
pmat = cor_pmat(df_trash)
ggcorrplot(corr_trash, type = "lower", lab = T, lab_size = 3)
```

calculate scores for scales & subscales
```{r}
df_scores = df_fa_imp %>%
  rowwise() %>%
  mutate(`TRASH consistency` = sum(
    trash_02_same_places,trash_04_sad_anx_annoyed,trash_06_when_alone,
    trash_08_same_times_day,trash_10_response_same_sit
  )) %>%
  mutate(`TRASH automaticity` = sum(
    trash_01_automatically,trash_03_hard_not_to,trash_05_unintentionally,
    trash_07_effort_to_stop,trash_09_start_before_realize
  )) %>%
  mutate(`RRS total` = mean(
    c(rrs_01,rrs_02,rrs_03,rrs_04,rrs_05,rrs_06,rrs_07,rrs_08,rrs_09,rrs_10,
    rrs_11,rrs_12,rrs_13,rrs_14,rrs_15,rrs_16,rrs_17,rrs_18,rrs_19,rrs_20,
    rrs_21,rrs_22)
  )*22) %>%
  mutate(`RRS depression` = mean(
    c(rrs_01,rrs_02,rrs_03,rrs_04,rrs_06,rrs_08,rrs_09,rrs_14 ,rrs_17,rrs_18,
    rrs_19,rrs_22)
  )*12) %>%
  mutate(`RRS brooding` = mean(
    c(rrs_05,rrs_10,rrs_13,rrs_15,rrs_16)
  )*5) %>%
  mutate(`RRS reflection` = mean(
    c(rrs_07,rrs_11,rrs_12,rrs_20,rrs_21)
  )*5) %>%

  mutate(`SARI anger` = sum(
    sari_01_angry,sari_02_angry,sari_03_angry,sari_04_angry,
    sari_05_angry,sari_06_angry,sari_07_angry,sari_08_angry,
    sari_09_angry,sari_10_angry,sari_11_angry
  )) %>%
  mutate(`SARI sadness` = sum(
    sari_01_sad,sari_02_sad,sari_03_sad,sari_04_sad,sari_05_sad,
    sari_06_sad,sari_07_sad,sari_08_sad,sari_09_sad,sari_10_sad,
    sari_11_sad
  )) %>%
  mutate(`rumVAS frequency` = rum_vas_01_frequency) %>%
  mutate(`rumVAS control` = rum_vas_02_control) %>%
  mutate(`rumVAS duration` = rum_vas_03_duration) %>%
  mutate(`rumVAS distress` = rum_vas_04_distress) %>%
  # relocate(rum_vas_01_frequency, .after = sari_sadness) %>%
  # relocate(rum_vas_02_control, .after = rum_vas_01_frequency) %>%
  # relocate(rum_vas_03_duration, .after = rum_vas_02_control) %>%
  # relocate(rum_vas_04_distress, .after = rum_vas_03_duration) %>%
  mutate(`PSWQ total` = sum(
    pswq_01_worries_bother_me,pswq_02_dont_worry_things,
    pswq_03_things_worry_me,pswq_04_cant_help_worry,
    pswq_05_worry_under_pressure,pswq_06_always_worrying,
    pswq_06_always_worrying,pswq_07_easy_stop_worrying,
    pswq_08_worry,pswq_09_never_worry,pswq_10_worry_all_my_life,
    pswq_11_notice_i_worry,pswq_12_start_worry_cant_stop,
    pswq_13_worry_all_time,pswq_14_worry_till_thngs_done
  )) %>%
  
  mutate(`CDRS total` = sum(
    cdrs01s,cdrs02s,cdrs03s,cdrs04s,cdrs05s,cdrs06s,cdrs07s,cdrs08s,cdrs09s,
    cdrs10s,cdrs11s,cdrs12s,cdrs13s,cdrs14s,cdrs15,cdrs16,cdrs17
  )) %>%
  
  mutate(`SCARED total` = sum(
    scared_01_hardtobreathe,scared_02_headaches_school,scared_03_dontlikeunknownppl,
    scared_04_scarednotsleepathome,scared_05_worrypeoplelikeme,scared_06_passingoutfrightened,
    scared_07_nervous,scared_08_follow_parents,scared_09_looknervous,scared_10_nervouswithppldontknow,
    scared_11_stomachachesschool,scared_12_frightened,scared_13_worrysleepalone,scared_14_worrybeinggood,
    scared_15_thingsunrealscared,scared_16_nightmaresparents,scared_17_worrygoingschool,
    scared_18_heartbeatsfastscared,scared_19_getshaky,scared_20_nightmaresbadtome,
    scared_21_worrythingsworkme,scared_22_sweatwhenscared,scared_23_worrier,
    scared_24_frightenednoreason,scared_25_afraidalonehouse,scared_26_hardtalktoppl,
    scared_27_chokingwhenscared,scared_28_peoplesayiworry,scared_29_dontlikeawayfamily,
    scared_30_afraidpanicattacks,scared_31_worryhappenstoparents,scared_32_shy,
    scared_33_worryfuture,scared_34_throwingupscared,scared_35_worryhowdothings,
    scared_36_scaredgotoschool,scared_37_worrypast+scared_38_dizzywhenscared,
    scared_39_nervouswithothers,scared_40_nervouspartiesdances,scared_41_shy
  )) %>%
  mutate(`SCARED panic` = sum(
    scared_01_hardtobreathe,scared_06_passingoutfrightened,scared_09_looknervous,
    scared_12_frightened,scared_15_thingsunrealscared,scared_18_heartbeatsfastscared,
    scared_19_getshaky,scared_22_sweatwhenscared,scared_24_frightenednoreason,
    scared_27_chokingwhenscared,scared_30_afraidpanicattacks,
    scared_34_throwingupscared,scared_38_dizzywhenscared
  )) %>%
  mutate(`SCARED GAD` = sum(
    scared_05_worrypeoplelikeme,scared_07_nervous,scared_14_worrybeinggood,
    scared_21_worrythingsworkme,scared_23_worrier,scared_28_peoplesayiworry,
    scared_33_worryfuture,scared_35_worryhowdothings,scared_37_worrypast
  )) %>%
  mutate(`SCARED sep anx` = sum(
    scared_04_scarednotsleepathome,scared_08_follow_parents,
    scared_13_worrysleepalone,scared_16_nightmaresparents,
    scared_20_nightmaresbadtome,scared_25_afraidalonehouse,
    scared_29_dontlikeawayfamily,scared_31_worryhappenstoparents
  )) %>%
  mutate(`SCARED social anx` = sum(
    scared_03_dontlikeunknownppl,scared_10_nervouswithppldontknow,
    scared_26_hardtalktoppl,scared_32_shy,scared_39_nervouswithothers,
    scared_40_nervouspartiesdances,scared_41_shy
  )) %>%
  mutate(`SCARED school avoid` = sum(
    scared_02_headaches_school,scared_11_stomachachesschool,
    scared_17_worrygoingschool,scared_36_scaredgotoschool
  )) %>%
  mutate(`MEAQ-A dis aver` = sum(
    meaq_01_free_pain,meaq_02_not_feel_bad,meaq_03_pain_suffering,
    meaq_04_never_feel_pain,meaq_05_rid_neg_emotions,
    meaq_06_anything_feel_btr,meaq_07_remove_pain_mem
  )) %>%
  mutate(`MEAQ-A dis supress` = sum(
    meaq_08_no_upset_feelings,meaq_09_no_unple_mems,
    meaq_10_not_think_upset,meaq_11_avoid_neg_thoughts,
    meaq_12_think_smthng_else
  )) %>%
  mutate(`MEAQ-A repress deny` = sum(
    meaq_13_hard_know_feeling,meaq_14_disconnect_emo,
    meaq_15_diff_how_feeling
  )) %>%
  mutate(`MEAQ-A repress deny eff` = sum(
    meaq_16_turn_off_emotions,meaq_17_numb_feelings
  )) %>%
  mutate(`MWDS deliberate` = sum(
    mwds_01d_wanderingthoughts,mwds_02d_enjoymindwandering,
    mwds_03d_boredom,mwds_04d_absorbedinfantasy
  )) %>%
  mutate(`MWDS spontaneous` = sum(
    mwds_01s_wanderingthoughts,mwds_02s_pulledfromtopic,
    mwds_03s_nocontrol,mwds_04s_mindwandering
  )) %>%
  mutate(`BIS total` = sum(
    bisbas_02_rarely_fear_nervous,bisbas_08_critique_hurts,
    bisbas_13_worry_upset_some1_mad,bisbas_16_work_up_unpleasant,
    bisbas_19_worry_do_poorly,bisbas_22_few_fears,
    bisbas_24_worry_mistakes
  )) %>%
  mutate(`UPPS-P-C lack premed` = sum(
    upps_04_blurtout,upps_06_stopandthink,upps_10_whattodo,upps_16_carefulapproach,
    upps_23_careful,upps_25_knowwhattoexpect,upps_28_stopandthink,upps_29_choices
  )) %>%
  mutate(`UPPS-P-C neg urgency` = sum(
    upps_01_doingsomething,upps_07_feelbad,upps_11_feelbad,upps_17_actwithoutthinking,
    upps_20_rejected,upps_26_actwithoutthinking,upps_30_mad,upps_32_crazythings
  )) %>%
  mutate(`UPPS-P-C sen seeking` = sum(
    upps_02_thrillingthings,upps_08_waterskiing,upps_12_takingrisks,upps_14_parachutejumping,
    upps_18_thrillingthings,upps_21_fly,upps_27_ski,upps_31_drivingfast
  )) %>%
  mutate(`UPPS-P-C lack perserve` = sum(
    upps_03_seethings,upps_05_upsetunfinished,upps_09_hatetostop,upps_13_thinkhard,
    upps_15_finish,upps_19_getthingsdone,upps_22_getsthejobdone,upps_24_finish
  )) %>%
  mutate(`UPPS-P-C pos urgency` = sum(
    upps_33_veryhappy,upps_34_thrilled,upps_35_greatmood,upps_36_actwithoutthinking,
    upps_37_troublehappy,upps_38_outofcontrol,upps_39_losecontrol,upps_40_problemsinlife
  )) %>%
  ungroup() %>%
  select(225:256)

df_scores
```


TODO: group rumination scales at the beginning
```{r,fig.height=40,fig.width=40}
library(ggcorrplot)

corr = round(cor(df_scores),2)
write.csv(corr,file = "scale_subscale_corr.csv")
pmat = cor_pmat(df_scores)
ggcorrplot(corr, type = "lower", hc.order = F, lab = T, lab_size = 10) +
  ggtitle("Scales & subscales correlations") +
  theme(
    axis.text.x = element_text(size=35),
    axis.text.y = element_text(size=35),
    plot.title = element_text(size = 40),
    legend.key.size = unit(3, 'cm'),
    legend.title = element_text(size=30),
    legend.text = element_text(size=25)
        )
```











```{r}
# library(TAM)
# 
# tam.fa(df_fa_imp, irtmodel = "efa", dims = 112, nfactors = 7)

```



```{r}
# library(ltm, pos = .Machine$integer.max)
# 
# df_fa %>%
#   dplyr::select(matches("^trash")) %>%
#   #dplyr::select(-rrs_01) %>%
#   cronbach.alpha()
```



```{r}
# test = df_fa_imp %>% 
#   select(c(matches("^rrs"),matches("upps"),matches("^pswq"),matches("^trash"))) 

#print(names(test))

# jmv::efa(
#     data = df_fa_imp,
#     vars = names(df_fa_imp),
#     #extraction = "ml",
#     extraction = "minres",
#     rotation = "oblimin",
#     hideLoadings = 0.0,
#     sortLoadings = TRUE,
#     screePlot = TRUE,
#     eigen = TRUE,
#     factorCor = TRUE,
#     factorSummary = TRUE,
#     modelFit = TRUE)

```

```{r}
# test = df_fa_imp %>% 
#   select(c(matches("^rrs"),matches("upps"),matches("^pswq"),matches("^trash")))

# fa.parallel(df_fa_imp, fa="fa")
```

factanal won't converge when items >= observations
```{r}
# fit <- factanal(df_fa_imp, 7, rotation="oblimin")
# print(fit, digits=2, cutoff=0.3, sort=TRUE)
```